Environment:
	Python: 3.9.15
	PyTorch: 1.7.1
	Torchvision: 0.8.2
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.20.3
	PIL: 8.3.2
Args:
	algorithm: FineTuning
	checkpoint_freq: None
	data_dir: ./domainbed/data
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 11
	output_dir: ./domainbed/sweep/output/ef918eb6d69d2d7f29df03ba27200bdc
	save_model_every_checkpoint: False
	seed: 2128787343
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 2]
	trial_seed: 2
	uda_holdout_fraction: 0
Traceback (most recent call last):
  File "/home/Matheart/miniconda3/envs/statml/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/Matheart/miniconda3/envs/statml/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/Matheart/Code/DomainBed/domainbed/scripts/train.py", line 78, in <module>
    hparams = hparams_registry.random_hparams(args.algorithm, args.dataset,
  File "/home/Matheart/Code/DomainBed/domainbed/hparams_registry.py", line 190, in random_hparams
    return {a: c for a, (b, c) in _hparams(algorithm, dataset, seed).items()}
  File "/home/Matheart/Code/DomainBed/domainbed/hparams_registry.py", line 151, in _hparams
    _hparam('lr', 5e-5, lambda r: 10**r.uniform(-5, -3.5))
  File "/home/Matheart/Code/DomainBed/domainbed/hparams_registry.py", line 22, in _hparam
    assert(name not in hparams)
AssertionError
